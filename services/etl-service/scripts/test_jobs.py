#!/usr/bin/env python3
"""
ETL Service - Jobs Testing & Debugging Tool

This script helps you test connections and debug job execution for all integrations.

Usage:
    python scripts/test_jobs.py [options]

Examples:
    # üîó CONNECTION TESTING:
    python scripts/test_jobs.py --test-connection # Test API connections
    python scripts/test_jobs.py --test-scheduler  # Test scheduler configuration

    # üêõ JOB DEBUGGING:
    python scripts/test_jobs.py --manual          # Interactive manual debugging
    python scripts/test_jobs.py --auto            # Full job with monitoring
    python scripts/test_jobs.py --auto --debug    # Full job with verbose logging
    python scripts/test_jobs.py --breakpoint      # Run with Python debugger breakpoint
"""

import sys
import argparse
import logging
import tempfile
import json
from pathlib import Path
from datetime import datetime, timedelta

# Add parent directory to Python path for imports
parent_dir = Path(__file__).parent.parent
sys.path.insert(0, str(parent_dir))

# Setup logging early to suppress SQLAlchemy logs before any imports
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
# Disable SQLAlchemy logging for cleaner output
logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)
logging.getLogger('sqlalchemy.pool').setLevel(logging.WARNING)
logging.getLogger('sqlalchemy.dialects').setLevel(logging.WARNING)

# Override DEBUG setting to disable SQLAlchemy echo for test_jobs
import os
os.environ['DEBUG'] = 'false'

# Fix Unicode encoding issues on Windows console
if sys.platform == "win32":
    try:
        # Try to set console to UTF-8 encoding
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')
    except:
        # If that fails, we'll handle it in the logging configuration
        pass

def setup_logging(debug=False):
    """Setup logging configuration using the main app's colored logging."""
    try:
        # Import and use the main app's colored logging configuration
        from app.core.logging_config import setup_logging as app_setup_logging
        app_setup_logging(force_reconfigure=True)  # Force reconfiguration
        print("‚úÖ Using colored logging configuration")
    except ImportError as e:
        # Fallback to basic logging if app modules aren't available
        level = logging.DEBUG if debug else logging.INFO
        logging.basicConfig(
            level=level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(sys.stdout)
            ]
        )
        print(f"‚ö†Ô∏è  Using basic logging configuration (colored logging not available: {e})")
    except Exception as e:
        print(f"‚ö†Ô∏è  Error setting up colored logging: {e}")
        # Fallback to basic logging
        level = logging.DEBUG if debug else logging.INFO
        logging.basicConfig(
            level=level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(sys.stdout)
            ]
        )

    # Disable SQLAlchemy logging for cleaner output
    logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)
    logging.getLogger('sqlalchemy.pool').setLevel(logging.WARNING)
    logging.getLogger('sqlalchemy.dialects').setLevel(logging.WARNING)

def test_connection():
    """Test API connections for all integrations."""
    print("üîó Testing API Connections...")
    print("=" * 60)
    
    try:
        from app.core.database import get_database
        from app.models.unified_models import Integration
        
        database = get_database()
        
        with database.get_session_context() as session:
            integrations = session.query(Integration).filter(Integration.active == True).all()
            
            if not integrations:
                print("‚ùå No active integrations found in database")
                return False
            
            print(f"üìä Found {len(integrations)} active integrations")
            
            for integration in integrations:
                print(f"\nüîç Testing {integration.name} integration...")
                
                if integration.name.lower() == 'jira':
                    success = test_jira_connection(integration)
                elif integration.name.lower() == 'github':
                    success = test_github_connection(integration)
                elif integration.name.lower() == 'aha!':
                    success = test_aha_connection(integration)
                elif integration.name.lower() == 'azure devops':
                    success = test_azure_devops_connection(integration)
                else:
                    print(f"   ‚ö†Ô∏è  Unknown integration type: {integration.name}")
                    continue
                
                if success:
                    print(f"   ‚úÖ {integration.name} connection successful")
                else:
                    print(f"   ‚ùå {integration.name} connection failed")
            
            return True
            
    except Exception as e:
        print(f"‚ùå Connection test failed: {e}")
        return False

def test_jira_connection(integration):
    """Test Jira API connection."""
    try:
        from app.jobs.jira import JiraAPIClient
        from app.core.config import AppConfig

        # Decrypt the token and extract connection details
        key = AppConfig.load_key()
        decrypted_token = AppConfig.decrypt_token(integration.password, key)
        username = integration.username
        base_url = integration.url

        client = JiraAPIClient(username, decrypted_token, base_url)

        # Test basic API call by getting projects
        projects = client.get_projects(max_results=1)
        if projects is not None:
            print(f"   üë§ Connected successfully - found {len(projects)} project(s)")
            return True
        else:
            print("   ‚ùå Failed to get projects")
            return False

    except Exception as e:
        print(f"   ‚ùå Jira connection error: {e}")
        return False

def test_github_connection(integration):
    """Test GitHub API connection."""
    try:
        from app.jobs.github import GitHubClient
        from app.core.config import get_settings
        
        settings = get_settings()
        github_token = getattr(settings, 'github_token', None)
        
        if not github_token:
            print("   ‚ùå GitHub token not configured")
            return False
        
        client = GitHubClient(github_token)
        
        # Test basic API call
        response = client._make_request("user")
        if response and 'login' in response:
            print(f"   üë§ Connected as: {response.get('login', 'Unknown')}")
            return True
        else:
            print("   ‚ùå Failed to get user info")
            return False
            
    except Exception as e:
        print(f"   ‚ùå GitHub connection error: {e}")
        return False

def test_aha_connection(integration):
    """Test Aha! API connection."""
    print("   ‚ö†Ô∏è  Aha! connection testing not yet implemented")
    return True

def test_azure_devops_connection(integration):
    """Test Azure DevOps API connection."""
    print("   ‚ö†Ô∏è  Azure DevOps connection testing not yet implemented")
    return True

def test_scheduler():
    """Test scheduler configuration."""
    print("üïê Testing Scheduler Configuration...")
    print("=" * 60)

    try:
        from app.main import get_scheduler

        scheduler = get_scheduler()

        if not scheduler:
            print("‚ö†Ô∏è  Scheduler not available (APScheduler not installed)")
            return True

        print("‚úÖ Scheduler imported successfully")
        print(f"üìä Scheduler state: {'Running' if scheduler.running else 'Stopped'}")

        # List configured jobs
        jobs = scheduler.get_jobs()
        print(f"üìã Configured jobs: {len(jobs)}")

        for job in jobs:
            print(f"   ‚Ä¢ {job.id}: {job.name}")
            print(f"     Next run: {job.next_run_time}")

        return True

    except Exception as e:
        print(f"‚ùå Scheduler test failed: {e}")
        return False

def manual_debug():
    """Interactive manual debugging mode with unified menu."""
    print("üêõ Manual Debugging Mode")
    print("=" * 60)

    # Setup colored logging BEFORE importing any app modules
    setup_logging(debug=True)

    try:
        from app.core.database import get_database
        from app.models.unified_models import Integration

        database = get_database()

        # Get available integrations once (outside the loop to avoid logs)
        with database.get_session_context() as session:
            integrations = session.query(Integration).filter(Integration.active == True).all()

            if not integrations:
                print("‚ùå No active integrations found")
                return

            # Create integration lookup with just the data we need (not the ORM objects)
            integration_data = []
            for integration in integrations:
                integration_data.append({
                    'id': integration.id,
                    'name': integration.name,
                    'url': integration.url,
                    'username': integration.username,
                    'password': integration.password,
                    'client_id': integration.client_id
                })

        # Create integration lookup from the data
        integration_lookup = {data['name'].lower(): data for data in integration_data}

        while True:
            try:
                print("\nÔøΩ Available ETL Operations:")
                print("=" * 50)

                # Jira operations
                if 'jira' in integration_lookup:
                    print("üìä Jira Operations:")
                    print("   1. Extract Projects and Issue Types (Combined)")
                    print("   2. Extract Projects and Statuses (Combined)")
                    print("   3. Extract Issues and Changelogs (Combined)")
                    print("   4. Run Full Jira Job")

                # GitHub operations
                if 'github' in integration_lookup:
                    print("\nüêô GitHub Operations:")
                    print("   5. Extract Repositories")
                    print("   6. Extract Pull Requests (All Repositories)")
                    print("   7. Extract Pull Requests (Single Repository)")
                    print("   8. Run Full GitHub Job (Real Job Simulation)")

                """
                # Other integrations
                if 'aha!' in integration_lookup:
                    print("\nüéØ Aha! Operations:")
                    print("   9. Run Aha! Extraction (Not Implemented)")

                if 'azure devops' in integration_lookup:
                    print("\nüî∑ Azure DevOps Operations:")
                    print("   10. Run Azure DevOps Extraction (Not Implemented)")
                """
                print("\n   q. Quit")

                choice = input(f"\nSelect operation (1-10) or 'q' to quit: ").strip()

                if choice.lower() == 'q':
                    break

                # Execute the selected operation
                if choice in ['1', '2', '3', '4'] and 'jira' in integration_lookup:
                    with database.get_session_context() as session:
                        # Get fresh integration object from the session
                        jira_integration = session.query(Integration).filter(Integration.id == integration_lookup['jira']['id']).first()
                        run_jira_operation(session, jira_integration, choice)
                elif choice in ['5', '6', '7', '8'] and 'github' in integration_lookup:
                    with database.get_session_context() as session:
                        # Get fresh integration object from the session
                        github_integration = session.query(Integration).filter(Integration.id == integration_lookup['github']['id']).first()
                        run_github_operation(session, github_integration, choice)
                elif choice == '9' and 'aha!' in integration_lookup:
                    print("‚ö†Ô∏è  Aha! extraction not yet implemented")
                elif choice == '10' and 'azure devops' in integration_lookup:
                    print("‚ö†Ô∏è  Azure DevOps extraction not yet implemented")
                else:
                    print("‚ùå Please enter a valid operation number")

            except ValueError:
                print("‚ùå Please enter a valid number")
            except KeyboardInterrupt:
                print("\nüëã Goodbye!")
                break
            except Exception as e:
                print(f"‚ùå Error: {e}")

    except Exception as e:
        print(f"‚ùå Manual debug failed: {e}")

def run_jira_operation(session, jira_integration, choice):
    """Run a specific Jira operation."""
    try:
        from app.jobs.jira import JiraAPIClient
        from app.jobs.jira.jira_extractors import extract_projects_and_issuetypes, extract_projects_and_statuses, extract_work_items_and_changelogs
        from app.core.config import AppConfig
        from app.core.logging_config import JobLogger


        # Setup
        key = AppConfig.load_key()
        jira_token = AppConfig.decrypt_token(jira_integration.password, key)
        jira_client = JiraAPIClient(
            username=jira_integration.username,
            token=jira_token,
            base_url=jira_integration.url
        )
        logger = JobLogger("manual_debug")

        if choice == '1':
            print("\nüîÑ Extracting Projects and Issue Types...")
            result = extract_projects_and_issuetypes(session, jira_client, jira_integration, logger)
            print(f"‚úÖ Result: {result['projects_processed']} projects processed")
            print(f"‚úÖ Result: {result['issuetypes_processed']} issue types processed")

        elif choice == '2':
            print("\nüîÑ Extracting Projects and Statuses...")
            result = extract_projects_and_statuses(session, jira_client, jira_integration, logger)
            print(f"‚úÖ Result: {result['statuses_processed']} statuses processed")
            print(f"‚úÖ Result: {result['relationships_processed']} project-status relationships processed")

        elif choice == '3':
            print("\nüîÑ Extracting Issues and Changelogs...")

            # Clear staging table before extraction to avoid duplicates
            from app.models.unified_models import JiraDevDetailsStaging, Issue
            staging_count = session.query(JiraDevDetailsStaging).join(Issue).filter(
                Issue.integration_id == jira_integration.id
            ).count()

            if staging_count > 0:
                print(f"üßπ Clearing {staging_count} existing dev_status staging records...")
                session.query(JiraDevDetailsStaging).filter(
                    JiraDevDetailsStaging.issue_id.in_(
                        session.query(Issue.id).filter(Issue.client_id == jira_integration.client_id)
                    )
                ).delete(synchronize_session=False)
                session.commit()
                print(f"‚úÖ Cleared existing staging records")

            result = extract_work_items_and_changelogs(session, jira_client, jira_integration, logger)
            print(f"‚úÖ Result: {result['issues_processed']} issues processed")
            print(f"‚úÖ Result: {result['changelogs_processed']} changelogs processed")
            print(f"‚úÖ Result: {result.get('dev_status_staged', 0)} dev_status records staged")
            print(f"üìä Issue keys: {len(result['issue_keys'])} keys collected for development data processing")

        elif choice == '4':
            print("\nüîÑ Running Full Jira Job via New Orchestration...")
            from app.jobs.orchestrator import trigger_jira_sync
            import asyncio
            result = asyncio.run(trigger_jira_sync())
            print(f"‚úÖ Full job result: {result}")

    except Exception as e:
        print(f"‚ùå Error in Jira operation: {e}")
        import traceback
        traceback.print_exc()


def run_github_operation(session, github_integration, choice):
    """Run a specific GitHub operation."""
    try:
        from app.core.config import AppConfig
        from app.core.logging_config import JobLogger

        # Setup
        key = AppConfig.load_key()
        github_token = AppConfig.decrypt_token(github_integration.password, key)
        logger = JobLogger("manual_debug")

        if choice == '5':
            print("\nüîÑ Extracting Repositories...")
            extract_repositories_manual(session, github_integration, github_token, logger)

        elif choice == '6':
            print("\nüîÑ Extracting Pull Requests (All Repositories)...")
            extract_all_pull_requests_manual(session, github_integration, github_token, logger)

        elif choice == '7':
            print("\nüîÑ Extracting Pull Requests (Single Repository)...")
            repo_full_name = input("Enter repository full name (owner/repo): ").strip()
            if repo_full_name:
                extract_single_repo_pull_requests_manual(session, github_integration, github_token, repo_full_name, logger)
            else:
                print("‚ùå Repository name required")

        elif choice == '8':
            print("\nÔøΩ Running Full GitHub Job (Real Job Simulation)...")
            run_full_github_extraction_manual(session, github_integration, github_token, logger)

    except Exception as e:
        print(f"‚ùå Error in GitHub operation: {e}")
        import traceback
        traceback.print_exc()

# Old run_jira_debug function removed - replaced by run_jira_operation in unified menu

# Old run_github_debug function removed - replaced by run_github_operation in unified menu

def auto_debug(debug=False):
    """Automatic debugging mode - runs full job with monitoring."""
    print("ü§ñ Automatic Debugging Mode")
    print("=" * 60)

    setup_logging(debug)

    try:
        from app.jobs.orchestrator import trigger_jira_sync
        import asyncio

        print("üöÄ Starting full job execution via new orchestration...")
        start_time = datetime.now()

        result = asyncio.run(trigger_jira_sync())

        end_time = datetime.now()
        duration = end_time - start_time

        print(f"\n‚úÖ Job completed successfully!")
        print(f"‚è±Ô∏è  Duration: {duration}")
        print(f"üìä Results:")
        print(json.dumps(result, indent=2, default=str))

        return True

    except Exception as e:
        print(f"‚ùå Auto debug failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def breakpoint_debug():
    """Run with Python debugger breakpoint."""
    print("üîç Breakpoint Debugging Mode")
    print("=" * 60)
    print("üí° Python debugger will start. Use 'c' to continue, 'n' for next line, 'q' to quit.")

    try:
        breakpoint()  # Python 3.7+ built-in debugger

        from app.jobs.orchestrator import trigger_jira_sync
        import asyncio
        result = asyncio.run(trigger_jira_sync())

        print("‚úÖ Breakpoint debug completed!")
        print(f"üìä Result: {result}")
        return True

    except Exception as e:
        print(f"‚ùå Breakpoint debug failed: {e}")
        return False

def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="ETL Service Jobs Testing & Debugging Tool",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    python utils/test_jobs.py --test-connection    # Test API connections
    python utils/test_jobs.py --manual             # Interactive debugging
    python utils/test_jobs.py --auto --debug       # Full job with verbose logging
        """
    )

    parser.add_argument('--test-connection', action='store_true',
                       help='Test API connections for all integrations')
    parser.add_argument('--test-scheduler', action='store_true',
                       help='Test scheduler configuration')
    parser.add_argument('--manual', action='store_true',
                       help='Interactive manual debugging mode')
    parser.add_argument('--auto', action='store_true',
                       help='Automatic debugging mode (full job)')
    parser.add_argument('--breakpoint', action='store_true',
                       help='Run with Python debugger breakpoint')
    parser.add_argument('--debug', action='store_true',
                       help='Enable debug logging')
    parser.add_argument('--check-checkpoint', action='store_true',
                       help='Check checkpoint data in database')

    args = parser.parse_args()

    # If no arguments provided, show help
    if not any(vars(args).values()):
        parser.print_help()
        return

    success = True

    try:
        if args.test_connection:
            success &= test_connection()

        if args.test_scheduler:
            success &= test_scheduler()

        if args.manual:
            manual_debug()

        if args.auto:
            success &= auto_debug(args.debug)

        if args.breakpoint:
            success &= breakpoint_debug()

        if args.check_checkpoint:
            check_checkpoint_data()

        if success:
            print("\nüéâ All operations completed successfully!")
        else:
            print("\n‚ö†Ô∏è  Some operations failed. Check the output above.")
            sys.exit(1)

    except KeyboardInterrupt:
        print("\nüëã Interrupted by user. Goodbye!")
        sys.exit(0)
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def check_checkpoint_data():
    """Check the current checkpoint data in the database."""
    try:
        # Setup environment and imports
        import sys
        import os
        sys.path.insert(0, os.path.abspath('.'))

        from app.core.database import get_database
        from app.models.unified_models import JobSchedule
        import json

        database = get_database()
        with database.get_session_context() as session:
            github_job = session.query(JobSchedule).filter_by(job_name='github_sync').first()

            if github_job:
                print('=== GITHUB JOB CHECKPOINT DATA ===')
                print(f'Job Status: {github_job.status}')
                print(f'Error Message: {github_job.error_message}')
                print(f'Retry Count: {github_job.retry_count}')
                print(f'Last Repo Sync Checkpoint: {github_job.last_repo_sync_checkpoint}')
                print(f'Last PR Cursor: {github_job.last_pr_cursor}')
                print(f'Current PR Node ID: {github_job.current_pr_node_id}')
                print(f'Last Commit Cursor: {github_job.last_commit_cursor}')
                print(f'Last Review Cursor: {github_job.last_review_cursor}')
                print(f'Last Comment Cursor: {github_job.last_comment_cursor}')
                print(f'Last Review Thread Cursor: {github_job.last_review_thread_cursor}')

                print('\n=== REPO PROCESSING QUEUE ===')
                if github_job.repo_processing_queue:
                    queue = json.loads(github_job.repo_processing_queue)
                    print(f'Total repositories in queue: {len(queue)}')

                    finished_repos = [repo for repo in queue if repo.get("finished", False)]
                    pending_repos = [repo for repo in queue if not repo.get("finished", False)]

                    print(f'Finished repositories: {len(finished_repos)}')
                    print(f'Pending repositories: {len(pending_repos)}')

                    print('\nFirst 5 finished repositories:')
                    for repo in finished_repos[:5]:
                        print(f'  - {repo.get("full_name", "N/A")} (finished: {repo.get("finished", False)})')

                    print('\nFirst 5 pending repositories:')
                    for repo in pending_repos[:5]:
                        print(f'  - {repo.get("full_name", "N/A")} (finished: {repo.get("finished", False)})')
                else:
                    print('No repo processing queue found')

            else:
                print('No GitHub job found in database')

    except Exception as e:
        print(f"Error checking checkpoint data: {e}")


# GitHub Manual Operation Functions

def extract_repositories_manual(session, github_integration, github_token, logger):
    """Manually extract repositories combining GitHub Search API and Jira dev_status data."""
    try:
        from app.jobs.github import GitHubClient
        from app.models.unified_models import Repository, JiraDevDetailsStaging
        from app.jobs.github.github_graphql_processor import GitHubGraphQLProcessor

        from datetime import datetime
        import os

        print("üîç Starting repository extraction...")
        print("üìã Combining repositories from:")
        print("   ‚Ä¢ GitHub Search API (with 'health-' filter)")
        print("   ‚Ä¢ Jira dev_status staging data")

        # Setup GitHub client
        github_client = GitHubClient(github_token)

        # Get organization from environment
        org = os.getenv('GITHUB_ORG', 'wexinc')
        name_filter = os.getenv('GITHUB_REPO_FILTER', 'health-')

        # Step 1: Get repositories from Jira dev_status staging data
        print("\nüîç Step 1: Extracting repositories from Jira dev_status staging data...")

        staged_data = session.query(JiraDevDetailsStaging).filter(
            JiraDevDetailsStaging.processed == False
        ).all()

        print(f"üìã Found {len(staged_data)} staged dev_status items")

        repo_names_from_jira = set()
        for staging_record in staged_data:
            dev_data = staging_record.get_dev_status_data()
            detail = dev_data.get('detail', [])
            for detail_item in detail:
                pull_requests = detail_item.get('pullRequests', [])
                for pr_data in pull_requests:
                    repo_name = pr_data.get('repositoryName')
                    if repo_name:
                        repo_names_from_jira.add(repo_name)

        print(f"üìÅ Found {len(repo_names_from_jira)} unique repositories in Jira dev_status data")
        if repo_names_from_jira:
            print(f"   ‚Ä¢ Repository names: {', '.join(sorted(repo_names_from_jira))}")

        # Step 2: Get repositories from GitHub Search API
        print(f"\nüîç Step 2: Searching GitHub API for repositories in org: {org}")
        if name_filter:
            print(f"üìã Filtering by name: {name_filter}")

        # Use integration's last_sync_at as start date, today as end date
        # Ensure we don't use dates that are too old (GitHub Search API has limits)
        if github_integration.last_sync_at and github_integration.last_sync_at.year >= 2020:
            start_date = github_integration.last_sync_at.strftime('%Y-%m-%d')
        else:
            start_date = "2024-01-01"  # Default to recent date if last_sync_at is None or too old

        end_date = datetime.today().strftime('%Y-%m-%d')

        print(f"üìÖ Date range: {start_date} to {end_date}")
        if github_integration.last_sync_at:
            print(f"üìÖ Integration last sync: {github_integration.last_sync_at}")
        else:
            print("üìÖ No previous sync found, using default start date")

        # Search repositories with rate limit warning
        print("Note: GitHub Search API has a rate limit of 30 requests per minute")
        repos_from_search = github_client.search_repositories(org, start_date, end_date, name_filter)
        print(f"üìÅ Found {len(repos_from_search)} repositories from GitHub Search API")

        # Step 3: Combine both sources
        print(f"\nüîó Step 3: Combining repositories from both sources...")

        # Create a set of all repository names to fetch
        all_repo_names = set()

        # Add repositories from GitHub Search API
        for repo in repos_from_search:
            all_repo_names.add(repo['full_name'])

        # Add repositories from Jira dev_status (need to construct full names)
        for repo_name in repo_names_from_jira:
            # Assume they're in the same org if not already full name
            if '/' not in repo_name:
                full_name = f"{org}/{repo_name}"
            else:
                full_name = repo_name
            all_repo_names.add(full_name)

        print(f"üìä Total unique repositories to process: {len(all_repo_names)}")

        # Step 4: Fetch detailed repository data for all repositories
        print(f"\nüì• Step 4: Fetching detailed repository data...")

        all_repos = []

        # Add repos from search (already have detailed data)
        all_repos.extend(repos_from_search)

        # For repos from Jira that weren't found in search, fetch them individually
        search_repo_names = {repo['full_name'] for repo in repos_from_search}
        missing_repo_names = all_repo_names - search_repo_names

        if missing_repo_names:
            print(f"üìã Fetching {len(missing_repo_names)} additional repositories from Jira dev_status...")
            for full_name in missing_repo_names:
                try:
                    owner, repo_name = full_name.split('/', 1)
                    # Use the GitHub client's _make_request method to fetch individual repo
                    endpoint = f"repos/{owner}/{repo_name}"
                    repo_data = github_client._make_request(endpoint)
                    if repo_data:
                        all_repos.append(repo_data)
                        print(f"   ‚úÖ Fetched: {full_name}")
                    else:
                        print(f"   ‚ùå Not found: {full_name}")
                except Exception as e:
                    print(f"   ‚ùå Error fetching {full_name}: {e}")

        print(f"üìÅ Total repositories to process: {len(all_repos)}")

        if not all_repos:
            print("‚ö†Ô∏è  No repositories found from either source")
            return

        # Step 5: Process repositories using bulk operations
        print(f"\nüîÑ Step 5: Processing repositories for database insertion...")
        processor = GitHubGraphQLProcessor(github_integration, None)

        # Get existing repositories to avoid duplicates
        existing_repos = {
            repo.external_id: repo for repo in session.query(Repository).filter(
                Repository.client_id == github_integration.client_id
            ).all()
        }

        repos_to_insert = []
        repos_skipped = 0

        print(f"üîÑ Processing {len(all_repos)} repositories...")
        for repo_index, repo_data in enumerate(all_repos, 1):
            try:
                print(f"üìÅ Processing repository {repo_index}/{len(all_repos)}: {repo_data.get('full_name', 'unknown')}")

                # Check if repository already exists
                external_id = str(repo_data['id'])
                if external_id in existing_repos:
                    print(f"   ‚è≠Ô∏è  Repository already exists (ID: {existing_repos[external_id].id})")
                    repos_skipped += 1
                    continue

                # Process repository data
                repo_processed = processor.process_repository_data(repo_data)
                if repo_processed:
                    repos_to_insert.append(repo_processed)
                    print(f"   ‚úÖ Prepared for bulk insert")
                else:
                    print(f"   ‚ö†Ô∏è  Failed to process repository data")

            except Exception as e:
                print(f"   ‚ùå Error processing repository: {e}")
                continue

        # Perform bulk insert
        if repos_to_insert:
            print(f"\nüíæ Performing bulk insert of {len(repos_to_insert)} repositories...")
            session.bulk_insert_mappings(Repository, repos_to_insert)
            session.commit()
            print(f"‚úÖ Successfully inserted {len(repos_to_insert)} repositories")
        else:
            print(f"\n‚ö†Ô∏è  No new repositories to insert")

        print(f"\nüéâ Repository extraction completed!")
        print(f"   ‚Ä¢ Repositories processed: {len(repos_to_insert)}")
        print(f"   ‚Ä¢ Repositories skipped (already exist): {repos_skipped}")

    except Exception as e:
        print(f"‚ùå Error in repository extraction: {e}")
        session.rollback()
        raise


def extract_all_pull_requests_manual(session, github_integration, github_token, logger):
    """Manually extract pull requests for all repositories using GraphQL."""
    try:
        from app.jobs.github.github_graphql_client import GitHubGraphQLClient
        from app.jobs.github.github_graphql_extractor import process_repository_prs_with_graphql
        from app.models.unified_models import Repository

        print("üîÑ Starting pull request extraction for all repositories...")

        # Get all repositories
        repositories = session.query(Repository).filter(
            Repository.client_id == github_integration.client_id,
            Repository.active == True
        ).all()

        if not repositories:
            print("‚ö†Ô∏è  No repositories found. Run option 5 first to extract repositories.")
            return

        print(f"üìÅ Found {len(repositories)} repositories")

        # Setup GraphQL client
        from app.core.config import get_settings
        settings = get_settings()
        graphql_client = GitHubGraphQLClient(github_token, rate_limit_threshold=settings.GITHUB_RATE_LIMIT_THRESHOLD)

        # Create a mock job schedule for testing
        mock_job_schedule = type('MockJobSchedule', (), {
            'is_recovery_run': lambda self: False,
            'get_checkpoint_state': lambda self: {},
            'update_checkpoint': lambda self, checkpoint_data: None
        })()

        total_prs_processed = 0

        for repo_index, repository in enumerate(repositories, 1):  # Process all repositories
            try:
                owner, repo_name = repository.full_name.split('/', 1)
                print(f"\nüîÑ Processing repository {repo_index}/{len(repositories)}: {owner}/{repo_name}")

                # Process PRs for this repository
                result = process_repository_prs_with_graphql(
                    session, graphql_client, repository, owner, repo_name,
                    github_integration, mock_job_schedule
                )

                # Check if rate limit was reached during PR processing BEFORE marking as finished
                if result.get('rate_limit_reached', False):
                    print("‚ö†Ô∏è  Rate limit threshold reached during PR processing, stopping gracefully")
                    print(f"üìä Processed {repo_index - 1} repositories before hitting rate limit")
                    print(f"üìä Total PRs processed: {total_prs_processed}")
                    break

                if result['success']:
                    prs_processed = result['prs_processed']
                    total_prs_processed += prs_processed

                    # Commit the data for this repository
                    session.commit()
                    print(f"‚úÖ Processed {prs_processed} PRs for {owner}/{repo_name} - Data committed to database")
                else:
                    print(f"‚ùå Failed to process PRs for {owner}/{repo_name}: {result['error']}")

                # Check rate limit after processing and warn but continue
                if graphql_client.should_stop_for_rate_limit():
                    print("‚ö†Ô∏è  Rate limit threshold reached, but continuing extraction")

            except Exception as e:
                print(f"‚ùå Error processing repository {repository.full_name}: {e}")
                continue

        # Step 2: Link pull requests with Jira issues using staging data
        print(f"\nLinking pull requests with Jira issues...")

        from app.jobs.github.github_job import link_pull_requests_with_jira_issues
        linking_result = link_pull_requests_with_jira_issues(session, github_integration)

        if linking_result['success']:
            print(f"‚úÖ Successfully linked {linking_result['links_created']} pull requests with Jira issues")
        else:
            print(f"‚ö†Ô∏è  PR-Issue linking completed with warnings: {linking_result.get('error', 'Unknown error')}")
            print(f"   ‚Ä¢ Links created: {linking_result.get('links_created', 0)}")

        print(f"\nüéâ Pull request extraction and linking completed!")
        print(f"   ‚Ä¢ Total PRs processed: {total_prs_processed}")
        print(f"   ‚Ä¢ Repositories processed: {repo_index}")
        print(f"   ‚Ä¢ PR-Issue links created: {linking_result.get('links_created', 0)}")

    except Exception as e:
        print(f"‚ùå Error in pull request extraction: {e}")
        raise


def extract_single_repo_pull_requests_manual(session, github_integration, github_token, repo_full_name, logger):
    """Manually extract pull requests for a single repository using GraphQL."""
    try:
        from app.jobs.github.github_graphql_client import GitHubGraphQLClient
        from app.jobs.github.github_graphql_extractor import process_repository_prs_with_graphql
        from app.models.unified_models import Repository

        print(f"üîÑ Starting pull request extraction for repository: {repo_full_name}")

        # Find the repository
        repository = session.query(Repository).filter(
            Repository.full_name == repo_full_name,
            Repository.client_id == github_integration.client_id,
            Repository.active == True
        ).first()

        if not repository:
            print(f"‚ùå Repository '{repo_full_name}' not found in database.")
            print("   Run option 5 first to extract repositories, or check the repository name.")
            return

        print(f"üìÅ Found repository: {repository.full_name} (ID: {repository.id})")

        # Setup GraphQL client
        from app.core.config import get_settings
        settings = get_settings()
        graphql_client = GitHubGraphQLClient(github_token, rate_limit_threshold=settings.GITHUB_RATE_LIMIT_THRESHOLD)

        # Create a mock job schedule for testing
        mock_job_schedule = type('MockJobSchedule', (), {
            'is_recovery_run': lambda self: False,
            'get_checkpoint_state': lambda self: {},
            'update_checkpoint': lambda self, checkpoint_data: None
        })()

        try:
            owner, repo_name_only = repo_full_name.split('/', 1)
            print(f"üîÑ Processing PRs for {owner}/{repo_name_only}...")

            # Process PRs for this repository
            result = process_repository_prs_with_graphql(
                session, graphql_client, repository, owner, repo_name_only,
                github_integration, mock_job_schedule
            )

            if result['success']:
                prs_processed = result['prs_processed']

                # Commit the data
                session.commit()

                # Step 2: Link pull requests with Jira issues using staging data
                print(f"\nLinking pull requests with Jira issues...")

                from app.jobs.github.github_job import link_pull_requests_with_jira_issues
                linking_result = link_pull_requests_with_jira_issues(session, github_integration)

                if linking_result['success']:
                    print(f"‚úÖ Successfully linked {linking_result['links_created']} pull requests with Jira issues")
                else:
                    print(f"‚ö†Ô∏è  PR-Issue linking completed with warnings: {linking_result.get('error', 'Unknown error')}")
                    print(f"   ‚Ä¢ Links created: {linking_result.get('links_created', 0)}")

                print(f"\nüéâ Pull request extraction and linking completed!")
                print(f"   ‚Ä¢ PRs processed: {prs_processed}")
                print(f"   ‚Ä¢ Repository: {repo_full_name}")
                print(f"   ‚Ä¢ PR-Issue links created: {linking_result.get('links_created', 0)}")
                print(f"   ‚Ä¢ Data committed to database")
            else:
                print(f"‚ùå Failed to process PRs: {result['error']}")

        except ValueError:
            print(f"‚ùå Invalid repository name format. Expected 'owner/repo', got '{repo_full_name}'")

    except Exception as e:
        print(f"‚ùå Error in single repository PR extraction: {e}")
        raise


def run_full_github_extraction_manual(session, github_integration, github_token, logger=None):
    """
    Run full GitHub extraction simulating the real job behavior.

    This function:
    1. Extracts repositories and pull requests
    2. Links PRs with Jira issues using staging data
    3. On complete success: truncates staging table, updates integration last_sync_at
    4. On failure/rate limit: saves checkpoint state to JobSchedule
    5. Updates JobSchedule status appropriately
    """
    try:
        print("üöÄ Starting full GitHub extraction (REAL JOB SIMULATION)...")
        print("=" * 60)
        print("This will behave like the real GitHub job:")
        print("‚Ä¢ Extract repositories and pull requests")
        print("‚Ä¢ Link with Jira staging data")
        print("‚Ä¢ Update JobSchedule and Integration on success")
        print("‚Ä¢ Truncate staging table on complete success")
        print("‚Ä¢ Save checkpoints on failure/rate limit")
        print("=" * 60)

        # Get or create JobSchedule for github_sync
        from app.models.unified_models import JobSchedule, JiraDevDetailsStaging
        from app.core.utils import DateTimeHelper

        github_job = session.query(JobSchedule).filter(JobSchedule.job_name == 'github_sync').first()
        if not github_job:
            print("‚ö†Ô∏è  No github_sync JobSchedule found, creating one...")
            github_job = JobSchedule(
                job_name='github_sync',
                status='PENDING',
                client_id=github_integration.client_id
            )
            session.add(github_job)
            session.commit()
            print("‚úÖ Created github_sync JobSchedule")

        # Set job to RUNNING
        github_job.set_running()
        session.commit()
        print(f"üìä GitHub job status: {github_job.status}")

        # Step 1: Extract repositories and pull requests using the REAL GitHub job logic
        print("\nüìã Step 1: Running real GitHub job logic (repositories + pull requests)...")
        print("This includes:")
        print("   ‚Ä¢ Repository discovery from GitHub Search API ('health-' filter)")
        print("   ‚Ä¢ Repository discovery from Jira dev_status staging data")
        print("   ‚Ä¢ Fetching missing repositories individually")
        print("   ‚Ä¢ Pull request extraction using GraphQL")

        from app.jobs.github.github_job import process_github_data_with_graphql
        result = process_github_data_with_graphql(session, github_integration, github_token, github_job)

        if result['success']:
            # Step 2: Link pull requests with Jira issues using staging data
            print("\nüìã Step 2: Linking pull requests with Jira issues...")
            from app.jobs.github.github_job import link_pull_requests_with_jira_issues
            linking_result = link_pull_requests_with_jira_issues(session, github_integration)

            if linking_result['success']:
                result['pr_links_created'] = linking_result['links_created']
                print(f"‚úÖ Successfully linked {linking_result['links_created']} pull requests with Jira issues")
            else:
                print(f"‚ö†Ô∏è  PR-Issue linking completed with warnings: {linking_result.get('error', 'Unknown error')}")
                result['pr_links_created'] = linking_result.get('links_created', 0)

            # Check if this was a complete success (not partial or rate limited)
            is_complete_success = (
                not result.get('rate_limit_reached', False) and
                not result.get('partial_success', False)
            )

            if is_complete_success:
                # Complete Success: Clean up and finish like real job
                print("\nüéâ COMPLETE SUCCESS - Cleaning up and finishing...")

                # Clear checkpoint data
                github_job.clear_checkpoints()

                # Truncate staging table
                staging_count = session.query(JiraDevDetailsStaging).count()
                if staging_count > 0:
                    session.query(JiraDevDetailsStaging).delete()
                    print(f"üóëÔ∏è  Truncated {staging_count} staging table records")
                else:
                    print("‚ÑπÔ∏è  No staging records to truncate")

                # Update integration last_sync_at
                github_integration.last_sync_at = DateTimeHelper.now_utc()
                print(f"üïí Updated GitHub integration last_sync_at: {github_integration.last_sync_at}")

                # Set job to FINISHED
                github_job.set_finished()
                session.commit()

                print("‚úÖ GitHub extraction completed successfully (REAL JOB BEHAVIOR)")
                print(f"   ‚Ä¢ Repositories processed: {result['repos_processed']}")
                print(f"   ‚Ä¢ Pull requests processed: {result['prs_processed']}")
                print(f"   ‚Ä¢ PR-Issue links created: {result.get('pr_links_created', 0)}")
                print(f"   ‚Ä¢ Staging table cleared: {staging_count} records")
                print(f"   ‚Ä¢ Integration timestamp updated")
                print(f"   ‚Ä¢ Job status: FINISHED")

            else:
                # Partial Success or Rate Limit: Keep staging data, keep job PENDING
                print("\n‚ö†Ô∏è  PARTIAL SUCCESS OR RATE LIMIT - Preserving state...")

                # Keep GitHub job as PENDING for next run
                github_job.status = 'PENDING'
                session.commit()

                print("üìä GitHub extraction partially completed (REAL JOB BEHAVIOR)")
                print(f"   ‚Ä¢ Repositories processed: {result['repos_processed']}")
                print(f"   ‚Ä¢ Pull requests processed: {result['prs_processed']}")
                print(f"   ‚Ä¢ PR-Issue links created: {result.get('pr_links_created', 0)}")
                print(f"   ‚Ä¢ Staging data preserved for next run")
                print(f"   ‚Ä¢ Job status: PENDING (will resume)")
                if result.get('rate_limit_reached'):
                    print(f"   ‚Ä¢ Rate limit reached - checkpoints saved")

        else:
            # Failure: Set job back to PENDING with checkpoint
            print("\n‚ùå EXTRACTION FAILED - Saving checkpoint...")
            error_msg = result.get('error', 'Unknown error')
            checkpoint_data = result.get('checkpoint_data', {})

            github_job.set_pending_with_checkpoint(
                error_msg,
                repo_checkpoint=checkpoint_data.get('repo_checkpoint'),
                repo_queue=checkpoint_data.get('repo_queue'),  # Updated parameter name
                last_pr_cursor=checkpoint_data.get('last_pr_cursor'),
                current_pr_node_id=checkpoint_data.get('current_pr_node_id'),
                last_commit_cursor=checkpoint_data.get('last_commit_cursor'),
                last_review_cursor=checkpoint_data.get('last_review_cursor'),
                last_comment_cursor=checkpoint_data.get('last_comment_cursor'),
                last_review_thread_cursor=checkpoint_data.get('last_review_thread_cursor')
            )
            session.commit()

            print(f"üíæ GitHub extraction failed (REAL JOB BEHAVIOR)")
            print(f"   ‚Ä¢ Error: {error_msg}")
            print(f"   ‚Ä¢ Checkpoint data saved for recovery")
            print(f"   ‚Ä¢ Job status: PENDING (will retry)")

        # Show final summary
        print("\n" + "=" * 60)
        print("üìä FINAL SUMMARY:")

        from app.models.unified_models import Repository, PullRequest

        repo_count = session.query(Repository).filter(
            Repository.client_id == github_integration.client_id,
            Repository.active == True
        ).count()

        pr_count = session.query(PullRequest).join(Repository).filter(
            Repository.client_id == github_integration.client_id,
            Repository.active == True
        ).count()

        linked_pr_count = session.query(PullRequest).join(Repository).filter(
            Repository.client_id == github_integration.client_id,
            Repository.active == True,
            PullRequest.issue_id.isnot(None)
        ).count()

        staging_count = session.query(JiraDevDetailsStaging).count()

        print(f"   ‚Ä¢ Total repositories: {repo_count}")
        print(f"   ‚Ä¢ Total pull requests: {pr_count}")
        print(f"   ‚Ä¢ Pull requests linked to Jira issues: {linked_pr_count}")
        print(f"   ‚Ä¢ Staging records remaining: {staging_count}")
        print(f"   ‚Ä¢ Job status: {github_job.status}")
        print(f"   ‚Ä¢ Integration last_sync_at: {github_integration.last_sync_at}")
        print("=" * 60)

    except Exception as e:
        print(f"‚ùå Error in full GitHub extraction: {e}")

        # Set job back to PENDING on unexpected error
        try:
            github_job = session.query(JobSchedule).filter(JobSchedule.job_name == 'github_sync').first()
            if github_job:
                github_job.set_pending_with_checkpoint(str(e))
                session.commit()
                print(f"üíæ Job set to PENDING due to unexpected error")
        except:
            pass

        raise

if __name__ == "__main__":
    main()
