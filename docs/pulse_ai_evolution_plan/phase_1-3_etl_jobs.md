# Phase 1-3: ETL Jobs Compatibility

**Implemented**: NO ‚ùå
**Duration**: Days 5-6
**Priority**: CRITICAL
**Dependencies**: Phase 1-2 (Unified Models) must be completed
**Can Run Parallel With**: Phase 1-4 (Backend APIs)

## üéØ Objectives

1. **GitHub Job Compatibility**: Update all GitHub data processing to handle new schema
2. **Jira Job Compatibility**: Update all Jira data processing to handle new schema
3. **Schema Compatibility**: Ensure no crashes with vector columns and ML tables
4. **Data Integrity**: Maintain all existing extraction and transformation logic
5. **Error Prevention**: Graceful handling of new fields without data population

## üìã Implementation Tasks

### Task 1-3.1: GitHub Job Updates
**Files**: 
- `services/etl-service/app/core/jobs/github_job.py`
- `services/etl-service/app/core/extractors/github_extractor.py`
- `services/etl-service/app/core/transformers/github_transformer.py`

**Objective**: Handle new schema in all GitHub data processing

### Task 1-3.2: Jira Job Updates
**Files**:
- `services/etl-service/app/core/jobs/jira_job.py`
- `services/etl-service/app/core/extractors/jira_extractor.py`
- `services/etl-service/app/core/transformers/jira_transformer.py`

**Objective**: Handle new schema in all Jira data processing

### Task 1-3.3: ETL Job Testing & Validation
**Objective**: Verify ETL jobs work correctly with enhanced models

## üîß Implementation Details

### Simple Approach: Models Handle New Fields by Default

**Key Principle**: Since Phase 1-2 updates all models to default `embedding=None`, ETL jobs should work without modification.

**No Special Compatibility Code Needed** - Just normal model creation:

```python
# ETL jobs create models normally - embedding defaults to None
issue = Issue(
    key=issue_data['key'],
    summary=issue_data['summary'],
    description=issue_data.get('description'),
    priority=issue_data.get('priority'),
    status_name=issue_data['status_name'],
    issuetype_name=issue_data['issuetype_name'],
    project_id=issue_data['project_id'],
    client_id=self.client_id,
    active=True
    # embedding automatically defaults to None in the model
)
```

### Simplified GitHub Job Updates
```python
# services/etl-service/app/core/jobs/github_job.py

class GitHubJob(BaseJob):
    """GitHub job - works with enhanced models automatically"""

    def __init__(self, client_id: int, config: dict):
        super().__init__(client_id, config)
        # Existing initialization preserved - no changes needed

    async def process_pull_request_batch(self, pr_batch: List[Dict]) -> List[PullRequest]:
        """Process PR batch - models handle new fields automatically"""
        processed_prs = []

        for pr_data in pr_batch:
            try:
                # Create PR model normally - embedding defaults to None
                pr = PullRequest(
                    number=pr_data['number'],
                    title=pr_data['title'],
                    body=pr_data['body'],
                    state=pr_data['state'],
                    author_login=pr_data['author_login'],
                    base_branch=pr_data['base_branch'],
                    head_branch=pr_data['head_branch'],
                    created_at=pr_data['created_at'],
                    updated_at=pr_data['updated_at'],
                    merged_at=pr_data.get('merged_at'),
                    closed_at=pr_data.get('closed_at'),
                    commit_count=pr_data.get('commit_count', 0),
                    changed_files=pr_data.get('changed_files', 0),
                    additions=pr_data.get('additions', 0),
                    deletions=pr_data.get('deletions', 0),
                    review_cycles=pr_data.get('review_cycles', 0),
                    rework_commit_count=pr_data.get('rework_commit_count', 0),
                    repository_id=pr_data['repository_id'],
                    client_id=self.client_id,
                    active=True
                    # embedding automatically defaults to None in model
                )

                processed_prs.append(pr)

            except Exception as e:
                self.logger.error(f"Error processing PR {pr_data.get('number', 'unknown')}: {e}")
                continue

        self.logger.info(f"‚úÖ Processed {len(processed_prs)} pull requests with enhanced schema")
        return processed_prs
    
    async def process_pull_request_comment_batch(self, comment_batch: List[Dict]) -> List[PullRequestComment]:
        """Process PR comment batch - models handle new fields automatically"""
        processed_comments = []

        for comment_data in comment_batch:
            try:
                # Create comment model normally - embedding defaults to None
                comment = PullRequestComment(
                    external_id=comment_data['external_id'],
                    body=comment_data['body'],
                    author_login=comment_data['author_login'],
                    created_at=comment_data['created_at'],
                    updated_at=comment_data['updated_at'],
                    path=comment_data.get('path'),
                    line=comment_data.get('line'),
                    comment_type=comment_data.get('comment_type', 'general'),
                    pull_request_id=comment_data['pull_request_id'],
                    client_id=self.client_id,
                    active=True
                    # embedding automatically defaults to None in model
                )

                processed_comments.append(comment)

            except Exception as e:
                self.logger.error(f"Error processing comment {comment_data.get('external_id', 'unknown')}: {e}")
                continue

        self.logger.info(f"‚úÖ Processed {len(processed_comments)} PR comments with enhanced schema")
        return processed_comments
    
    async def process_repository_batch(self, repo_batch: List[Dict]) -> List[Repository]:
        """Process repository batch with schema compatibility"""
        processed_repos = []
        
        for repo_data in repo_batch:
            try:
                # Prepare data for new schema
                repo_data = self.prepare_data_for_new_schema(repo_data)
                
                repo = self.safe_model_creation(
                    Repository,
                    {
                        # Existing fields (all preserved)
                        'name': repo_data['name'],
                        'full_name': repo_data['full_name'],
                        'description': repo_data.get('description'),
                        'private': repo_data.get('private', False),
                        'default_branch': repo_data.get('default_branch', 'main'),
                        'language': repo_data.get('language'),
                        'created_at': repo_data.get('created_at'),
                        'updated_at': repo_data.get('updated_at'),
                        'client_id': self.client_id,
                        'active': True,
                        
                        # NEW: Schema compatibility field
                        'embedding': None  # Phase 1: Always None
                    },
                    context=f"Repository {repo_data.get('full_name', 'unknown')}"
                )
                
                processed_repos.append(repo)
                
            except Exception as e:
                self.logger.error(f"Error processing repository {repo_data.get('full_name', 'unknown')}: {e}")
                continue
        
        self.log_schema_compatibility("repositories", len(processed_repos))
        return processed_repos
```

### Simplified Jira Job Updates
```python
# services/etl-service/app/core/jobs/jira_job.py

class JiraJob(BaseJob):
    """Jira job - works with enhanced models automatically"""

    def __init__(self, client_id: int, config: dict):
        super().__init__(client_id, config)
        # Existing initialization preserved - no changes needed
    
    async def process_issue_batch(self, issue_batch: List[Dict]) -> List[Issue]:
        """Process issue batch - models handle new fields automatically"""
        processed_issues = []

        for issue_data in issue_batch:
            try:
                # Create issue model normally - embedding defaults to None
                issue = Issue(
                    key=issue_data['key'],
                    summary=issue_data['summary'],
                    description=issue_data.get('description'),
                    priority=issue_data.get('priority'),
                    status_name=issue_data['status_name'],
                    issuetype_name=issue_data['issuetype_name'],
                    assignee=issue_data.get('assignee'),
                    assignee_id=issue_data.get('assignee_id'),
                    reporter=issue_data.get('reporter'),
                    reporter_id=issue_data.get('reporter_id'),
                    created=issue_data['created'],
                    updated=issue_data['updated'],
                    resolved=issue_data.get('resolved'),
                    due_date=issue_data.get('due_date'),
                    story_points=issue_data.get('story_points'),
                    epic_link=issue_data.get('epic_link'),
                    parent_key=issue_data.get('parent_key'),
                    level_number=issue_data.get('level_number', 0),
                    work_started_at=issue_data.get('work_started_at'),
                    work_first_completed_at=issue_data.get('work_first_completed_at'),
                    work_last_completed_at=issue_data.get('work_last_completed_at'),
                    total_lead_time_seconds=issue_data.get('total_lead_time_seconds'),
                    project_id=issue_data['project_id'],
                    status_id=issue_data['status_id'],
                    issuetype_id=issue_data['issuetype_id'],
                    parent_id=issue_data.get('parent_id'),
                    comment_count=issue_data.get('comment_count', 0),
                    client_id=self.client_id,
                    active=True,
                    # Custom fields (existing)
                    custom_field_01=issue_data.get('custom_field_01'),
                    custom_field_02=issue_data.get('custom_field_02')
                    # ... (all 20 custom fields)
                    # embedding automatically defaults to None in model
                )

                processed_issues.append(issue)

            except Exception as e:
                self.logger.error(f"Error processing issue {issue_data.get('key', 'unknown')}: {e}")
                continue

        self.logger.info(f"‚úÖ Processed {len(processed_issues)} issues with enhanced schema")
        return processed_issues
    
    async def process_project_batch(self, project_batch: List[Dict]) -> List[Project]:
        """Process project batch with schema compatibility"""
        processed_projects = []
        
        for project_data in project_batch:
            try:
                # Prepare data for new schema
                project_data = self.prepare_data_for_new_schema(project_data)
                
                project = self.safe_model_creation(
                    Project,
                    {
                        # Existing fields (all preserved)
                        'key': project_data['key'],
                        'name': project_data['name'],
                        'description': project_data.get('description'),
                        'lead': project_data.get('lead'),
                        'project_type': project_data.get('project_type'),
                        'created_at': project_data.get('created_at'),
                        'updated_at': project_data.get('updated_at'),
                        'client_id': self.client_id,
                        'active': True,
                        
                        # NEW: Schema compatibility field
                        'embedding': None  # Phase 1: Always None
                    },
                    context=f"Project {project_data.get('key', 'unknown')}"
                )
                
                processed_projects.append(project)
                
            except Exception as e:
                self.logger.error(f"Error processing project {project_data.get('key', 'unknown')}: {e}")
                continue
        
        self.log_schema_compatibility("projects", len(processed_projects))
        return processed_projects
```

## ‚úÖ Success Criteria

1. **GitHub Job**: Runs successfully without crashes using enhanced models
2. **Jira Job**: Runs successfully without crashes using enhanced models
3. **Data Processing**: All existing extraction/transformation logic preserved
4. **Model Creation**: Models instantiate correctly with embedding=None by default
5. **Error Handling**: Existing error handling continues to work
6. **Performance**: No significant performance degradation
7. **Data Integrity**: All existing data processing unchanged
8. **Simplicity**: No special compatibility code needed - models handle it

## üìù Testing Checklist

- [ ] GitHub ETL job completes successfully
- [ ] Jira ETL job completes successfully
- [ ] All model instantiation includes embedding=None
- [ ] No crashes during data processing
- [ ] Existing data extraction unchanged
- [ ] Schema compatibility mixin works correctly
- [ ] Error handling prevents job failures
- [ ] Performance remains acceptable
- [ ] Logging shows compatibility status

## üîÑ Completion Enables

- **Phase 1-4**: Backend APIs can rely on ETL data with new schema
- **Phase 1-7**: Integration testing can validate ETL functionality
- **Phase 2**: ETL jobs ready for ML enhancement

## üìã Handoff to Phase 1-4

**Deliverables**:
- ‚úÖ GitHub job compatible with enhanced schema
- ‚úÖ Jira job compatible with enhanced schema
- ‚úÖ Schema compatibility utilities
- ‚úÖ Comprehensive error handling

**Next Phase Requirements**:
- Backend APIs can process data from enhanced ETL jobs
- Health checks can validate ETL job status
- Integration testing can verify end-to-end data flow
