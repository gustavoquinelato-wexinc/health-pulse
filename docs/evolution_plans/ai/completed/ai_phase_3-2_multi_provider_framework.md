# Phase 3-2: Hybrid AI Provider Framework (WEX Gateway + Direct Integration)

**Implemented**: YES âœ…
**Duration**: 2 days (Days 2-3 of 10)
**Priority**: CRITICAL
**Dependencies**: Phase 3-1 completion

## ðŸ’¼ Business Outcome

**Cost Optimization**: Reduce AI operational costs by 60-80% through intelligent routing between WEX Gateway (paid) and local Sentence Transformers (free) for embedding generation, while maintaining enterprise-grade reliability and performance for critical AI operations.

## ðŸŽ¯ Hybrid Approach Strategy

**Primary**: Use WEX AI Gateway (simpler, already configured)
**Secondary**: Direct integration with AI providers (future flexibility)

This approach leverages the existing `integrations` table structure and builds upon the current WEX AI Gateway implementation while keeping the door open for direct provider integration.

### **1. Current State Analysis**
âœ… **Already Implemented**:
- `integrations` table with AI provider support
- WEX AI Gateway integration configured
- Encrypted credential storage
- Fallback integration support
- Cost tracking structure (`cost_config` JSONB)
- Model configuration (`ai_model_config` JSONB)

### **2. What We Need to Add**
âŒ **Missing Components**:
- Provider abstraction layer for direct integrations
- Enhanced Qdrant client with performance optimization
- Intelligent provider selection logic
- Batch processing for embeddings
- Local model support (Sentence Transformers)

## ðŸŽ¯ Phase 3-2 Objectives

1. **Hybrid Provider Manager**: WEX AI Gateway as primary + direct provider support
2. **Enhanced Qdrant Client**: High-performance vector operations with client isolation
3. **Intelligent Provider Selection**: Smart routing between WEX Gateway and direct providers
4. **Performance Optimization**: 10x faster embedding generation with batching
5. **Local Model Support**: Sentence Transformers for zero-cost embeddings
6. **Backward Compatibility**: Preserve existing WEX AI Gateway functionality

## ðŸš€ Hybrid Provider Architecture

### **Current WEX AI Gateway Integration (Already Working)**
```python
# Current implementation - services/etl-service/app/core/wex_ai_integration.py
# This is already working and should be preserved

class WEXAIGatewayClient:
    """Existing WEX AI Gateway client - keep as primary provider"""

    def __init__(self, base_url: str, api_key: str):
        self.base_url = base_url
        self.api_key = api_key
        self.client = OpenAI(base_url=base_url, api_key=api_key)

    async def generate_embeddings(self, texts: List[str], model: str = "azure-text-embedding-3-small"):
        """Generate embeddings via WEX AI Gateway"""
        # Existing implementation - preserve this
        pass

    async def generate_text(self, prompt: str, model: str = "azure-gpt-4o-mini", **kwargs):
        """Generate text via WEX AI Gateway"""
        # Existing implementation - preserve this
        pass
```

### **New: Hybrid Provider Manager**
```python
# services/backend-service/app/ai/hybrid_provider_manager.py
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass
import asyncio
import logging

@dataclass
class ProviderConfig:
    """Simplified provider configuration using existing integrations table"""
    integration_id: int
    provider: str  # 'wex_ai_gateway', 'openai', 'sentence_transformers'
    type: str      # 'ai_provider'
    base_url: str
    api_key: str
    ai_model: str
    ai_model_config: Dict[str, Any]
    cost_config: Dict[str, Any]
    fallback_integration_id: Optional[int] = None

class HybridProviderManager:
    """Manages both WEX AI Gateway and direct provider integrations"""

    def __init__(self, db_session):
        self.db_session = db_session
        self.providers = {}
        self.wex_gateway_client = None
        self.local_models = {}

    async def initialize_providers(self, tenant_id: int):
        """Initialize providers from integrations table"""
        # Load AI provider integrations for this tenant
        integrations = self.db_session.query(Integration).filter(
            Integration.tenant_id == tenant_id,
            Integration.type == 'ai_provider',
            Integration.active == True
        ).all()

        for integration in integrations:
            if integration.provider == 'wex_ai_gateway':
                # Initialize WEX AI Gateway (primary)
                self.wex_gateway_client = WEXAIGatewayClient(
                    base_url=integration.base_url,
                    api_key=decrypt_password(integration.password)
                )
            elif integration.provider == 'sentence_transformers':
                # Initialize local models (zero cost)
                await self._initialize_local_model(integration)
            else:
                # Initialize direct provider (future)
                await self._initialize_direct_provider(integration)

    async def generate_embeddings(self, texts: List[str], tenant_id: int,
                                 preferred_provider: str = "auto") -> List[List[float]]:
        """Generate embeddings with intelligent provider selection"""

        if preferred_provider == "auto":
            # Smart selection: local models for small batches, WEX Gateway for large
            if len(texts) <= 10 and 'sentence_transformers' in self.providers:
                return await self._generate_local_embeddings(texts)
            else:
                return await self._generate_wex_gateway_embeddings(texts, tenant_id)
        elif preferred_provider == "wex_ai_gateway":
            return await self._generate_wex_gateway_embeddings(texts, tenant_id)
        elif preferred_provider == "sentence_transformers":
            return await self._generate_local_embeddings(texts)
        else:
            # Fallback to WEX Gateway
            return await self._generate_wex_gateway_embeddings(texts, tenant_id)
```

### **WEX AI Gateway Provider (Primary - Already Working)**
```python
# services/backend-service/app/ai/providers/wex_gateway_provider.py
# Enhanced version of existing WEX AI Gateway integration

class WEXGatewayProvider:
    """Enhanced WEX AI Gateway provider with batching and cost tracking"""

    def __init__(self, integration: Integration):
        self.integration = integration
        self.base_url = integration.base_url
        self.api_key = decrypt_password(integration.password)
        self.model_config = integration.ai_model_config or {}
        self.cost_config = integration.cost_config or {}

        # Initialize OpenAI client pointing to WEX Gateway
        self.client = AsyncOpenAI(
            base_url=self.base_url,
            api_key=self.api_key,
            timeout=self.model_config.get('timeout', 120)
        )

    async def generate_embeddings(self, texts: List[str], model: str = None) -> List[List[float]]:
        """Generate embeddings via WEX AI Gateway with batching"""
        if not texts:
            return []

        # Use model from integration or parameter
        model_name = model or self.integration.ai_model or "azure-text-embedding-3-small"
        batch_size = self.model_config.get('batch_size', 100)

        all_embeddings = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]

            try:
                response = await self.client.embeddings.create(
                    model=model_name,
                    input=batch
                )

                batch_embeddings = [item.embedding for item in response.data]
                all_embeddings.extend(batch_embeddings)

                # Track usage in existing ai_usage_trackings table
                await self._track_usage(
                    operation="embedding",
                    model_name=model_name,
                    input_count=len(batch),
                    input_tokens=response.usage.total_tokens if hasattr(response, 'usage') else 0
                )

            except Exception as e:
                logger.error(f"WEX Gateway embedding batch failed: {e}")
                # Return zero vectors for failed batch (1536 dimensions)
                all_embeddings.extend([[0.0] * 1536] * len(batch))

        return all_embeddings

    async def generate_text(self, prompt: str, model: str = None, **kwargs) -> str:
        """Generate text via WEX AI Gateway"""
        model_name = model or self.integration.ai_model or "azure-gpt-4o-mini"
        max_retries = self.model_config.get('max_retries', 3)

        for attempt in range(max_retries):
            try:
                response = await self.client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "user", "content": prompt}],
                    **{**self.model_config, **kwargs}
                )

                # Track usage
                await self._track_usage(
                    operation="text_generation",
                    model_name=model_name,
                    input_tokens=response.usage.prompt_tokens,
                    output_tokens=response.usage.completion_tokens,
                    total_tokens=response.usage.total_tokens
                )

                return response.choices[0].message.content

            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                await asyncio.sleep(2 ** attempt)

    async def _track_usage(self, operation: str, model_name: str, **kwargs):
        """Track usage in existing ai_usage_trackings table"""
        # Implementation to insert into ai_usage_trackings table
        pass
```

### **Sentence Transformers Provider (Local, Zero-Cost)**
```python
# services/backend-service/app/ai/providers/sentence_transformers_provider.py
from sentence_transformers import SentenceTransformer
import torch
from typing import List
import asyncio
import logging

logger = logging.getLogger(__name__)

class SentenceTransformersProvider:
    """Local sentence transformers for zero-cost embeddings"""

    def __init__(self, integration: Integration):
        self.integration = integration
        self.model_config = integration.ai_model_config or {}

        # Default to fast, lightweight model
        model_name = integration.ai_model or "all-MiniLM-L6-v2"

        # Load model with GPU support if available
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = SentenceTransformer(model_name, device=device)

        # Performance optimization for GPU
        if device == "cuda":
            self.model.half()  # Use half precision for speed

        logger.info(f"Loaded local model {model_name} on {device}")

    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Ultra-fast local embedding generation (1000+ embeddings/second)"""
        if not texts:
            return []

        try:
            # Run in thread pool to avoid blocking async loop
            loop = asyncio.get_event_loop()
            embeddings = await loop.run_in_executor(
                None,
                self._encode_texts,
                texts
            )

            # Track usage (zero cost for local models)
            await self._track_usage(
                operation="embedding",
                model_name=self.integration.ai_model,
                input_count=len(texts),
                cost=0.0  # Free local processing
            )

            return embeddings.tolist()

        except Exception as e:
            logger.error(f"SentenceTransformers embedding failed: {e}")
            # Return zero vectors with appropriate dimensions
            dimensions = self._get_model_dimensions()
            return [[0.0] * dimensions] * len(texts)

    def _encode_texts(self, texts: List[str]) -> List[List[float]]:
        """Synchronous encoding method for thread pool execution"""
        batch_size = self.model_config.get('batch_size', 32)

        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=False,
            convert_to_numpy=True,
            normalize_embeddings=True  # Better for similarity search
        )

        return embeddings

    def _get_model_dimensions(self) -> int:
        """Get embedding dimensions for the current model"""
        model_dimensions = {
            "all-MiniLM-L6-v2": 384,
            "all-mpnet-base-v2": 768,
            "all-distilroberta-v1": 768
        }
        return model_dimensions.get(self.integration.ai_model, 384)

    async def _track_usage(self, operation: str, model_name: str, **kwargs):
        """Track usage in existing ai_usage_trackings table"""
        # Implementation to insert into ai_usage_trackings table
        pass
```

### **Simplified Provider Factory (Using Existing Integrations)**
```python
# services/backend-service/app/ai/provider_factory.py
from typing import Dict, Type, Optional
from sqlalchemy.orm import Session
from ..models.unified_models import Integration
from .providers.wex_gateway_provider import WEXGatewayProvider
from .providers.sentence_transformers_provider import SentenceTransformersProvider

class HybridProviderFactory:
    """Factory for creating providers from existing integrations table"""

    def __init__(self, db_session: Session):
        self.db_session = db_session
        self._provider_cache = {}

    async def get_provider(self, tenant_id: int, provider_type: str = "auto"):
        """Get provider instance from integrations table"""

        # Check cache first
        cache_key = f"{tenant_id}_{provider_type}"
        if cache_key in self._provider_cache:
            return self._provider_cache[cache_key]

        if provider_type == "auto":
            # Smart selection: prefer WEX Gateway, fallback to local
            provider = await self._get_wex_gateway_provider(tenant_id)
            if not provider:
                provider = await self._get_local_provider(tenant_id)
        elif provider_type == "wex_ai_gateway":
            provider = await self._get_wex_gateway_provider(tenant_id)
        elif provider_type == "sentence_transformers":
            provider = await self._get_local_provider(tenant_id)
        else:
            raise ValueError(f"Unknown provider type: {provider_type}")

        # Cache the provider
        if provider:
            self._provider_cache[cache_key] = provider

        return provider

    async def _get_wex_gateway_provider(self, tenant_id: int) -> Optional[WEXGatewayProvider]:
        """Get WEX AI Gateway provider from integrations table"""
        integration = self.db_session.query(Integration).filter(
            Integration.tenant_id == tenant_id,
            Integration.provider == 'wex_ai_gateway',
            Integration.type == 'ai_provider',
            Integration.active == True
        ).first()

        if integration:
            return WEXGatewayProvider(integration)
        return None

    async def _get_local_provider(self, tenant_id: int) -> Optional[SentenceTransformersProvider]:
        """Get or create local Sentence Transformers provider"""
        integration = self.db_session.query(Integration).filter(
            Integration.tenant_id == tenant_id,
            Integration.provider == 'sentence_transformers',
            Integration.type == 'ai_provider',
            Integration.active == True
        ).first()

        if not integration:
            # Create default local provider integration
            integration = await self._create_default_local_integration(tenant_id)

        if integration:
            return SentenceTransformersProvider(integration)
        return None

    async def _create_default_local_integration(self, tenant_id: int) -> Integration:
        """Create default local Sentence Transformers integration"""
        integration = Integration(
            tenant_id=tenant_id,
            provider='sentence_transformers',
            type='ai_provider',
            ai_model='all-MiniLM-L6-v2',
            ai_model_config={'batch_size': 32, 'normalize_embeddings': True},
            cost_config={'cost_per_1k_tokens': 0.0},
            active=True
        )

        self.db_session.add(integration)
        self.db_session.commit()

        return integration
```

### **Qdrant Client (Enterprise-Grade)**
```python
# services/backend-service/app/ai/qdrant_client.py
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition
import uuid
from typing import List, Dict, Any, Optional
import asyncio

class PulseQdrantClient:
    """Enterprise Qdrant client with client isolation and performance optimization"""
    
    def __init__(self):
        self.client = QdrantClient(
            host=os.getenv("QDRANT_HOST", "localhost"),
            port=int(os.getenv("QDRANT_PORT", "6333")),
            timeout=int(os.getenv("QDRANT_TIMEOUT", "120"))
        )
    
    def _get_collection_name(self, client_id: int, table_name: str) -> str:
        """Generate client-specific collection name for perfect isolation"""
        return f"client_{client_id}_{table_name}"
    
    async def create_collection(self, client_id: int, table_name: str, vector_size: int = 1536):
        """Create client-specific collection with performance optimization"""
        collection_name = self._get_collection_name(client_id, table_name)
        
        try:
            await self.client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(
                    size=vector_size,
                    distance=Distance.COSINE,
                    # Performance optimization for 10M+ scale
                    hnsw_config={
                        "m": 16,
                        "ef_construct": 100,
                        "full_scan_threshold": 10000
                    }
                )
            )
            logger.info(f"Created Qdrant collection: {collection_name}")
        except Exception as e:
            if "already exists" not in str(e):
                logger.error(f"Failed to create collection {collection_name}: {e}")
                raise
    
    async def upsert_vectors_batch(self, client_id: int, table_name: str, 
                                  records: List[Dict[str, Any]]) -> List[str]:
        """Batch upsert for high performance"""
        collection_name = self._get_collection_name(client_id, table_name)
        
        points = []
        point_ids = []
        
        for record in records:
            point_id = str(uuid.uuid4())
            point_ids.append(point_id)
            
            # Add client_id to metadata for additional filtering
            metadata = record.get("metadata", {})
            metadata.update({
                "client_id": client_id,
                "table_name": table_name,
                "record_id": record["record_id"]
            })
            
            points.append(PointStruct(
                id=point_id,
                vector=record["vector"],
                payload=metadata
            ))
        
        # Batch upsert for performance
        await self.client.upsert(
            collection_name=collection_name,
            points=points
        )
        
        return point_ids
    
    async def search_similar(self, client_id: int, table_name: str, 
                           query_vector: List[float], limit: int = 10,
                           score_threshold: float = 0.7) -> List[Dict]:
        """High-performance similarity search with client isolation"""
        collection_name = self._get_collection_name(client_id, table_name)
        
        # Double-layer client isolation: collection name + filter
        filter_condition = Filter(
            must=[
                FieldCondition(key="client_id", match={"value": client_id})
            ]
        )
        
        results = await self.client.search(
            collection_name=collection_name,
            query_vector=query_vector,
            query_filter=filter_condition,
            limit=limit,
            score_threshold=score_threshold,
            # Performance optimization
            search_params={"hnsw_ef": 128}
        )
        
        return [
            {
                "id": result.id,
                "score": result.score,
                "metadata": result.payload,
                "record_id": result.payload.get("record_id"),
                "table_name": result.payload.get("table_name")
            }
            for result in results
        ]
```

## ðŸ“‹ Implementation Tasks

### **Task 3-2.1: Enhanced WEX Gateway Provider**
- [ ] Enhance existing WEX AI Gateway client with batching
- [ ] Add cost tracking to ai_usage_trackings table
- [ ] Implement retry logic and error handling
- [ ] Add model selection based on task complexity

### **Task 3-2.2: Local Sentence Transformers Provider**
- [ ] Implement SentenceTransformersProvider class
- [ ] Add GPU support and performance optimization
- [ ] Create async wrapper for thread pool execution
- [ ] Add automatic model downloading and caching

### **Task 3-2.3: Hybrid Provider Manager**
- [ ] Create HybridProviderManager class
- [ ] Implement intelligent provider selection logic
- [ ] Add provider caching and connection pooling
- [ ] Integrate with existing integrations table

### **Task 3-2.4: Enhanced Qdrant Client**
- [ ] Create PulseQdrantClient with tenant isolation
- [ ] Implement batch operations for performance
- [ ] Add collection management with naming conventions
- [ ] Implement similarity search with filtering

### **Task 3-2.5: Provider Factory Integration**
- [ ] Create HybridProviderFactory using integrations table
- [ ] Add automatic local provider creation
- [ ] Implement provider caching and lifecycle management
- [ ] Add health checks and failover logic

## âœ… Success Criteria

1. **Hybrid Architecture**: WEX Gateway as primary, local models as secondary
2. **Backward Compatibility**: Existing WEX AI Gateway functionality preserved
3. **Performance**: 10x improvement in embedding generation with batching
4. **Zero-Cost Option**: Local Sentence Transformers working
5. **Tenant Isolation**: Perfect separation in Qdrant collections
6. **Cost Tracking**: Usage monitoring in existing ai_usage_trackings table

## ðŸ”„ Completion Enables

- **Phase 3-3**: Frontend AI configuration interface
- **Phase 3-4**: ETL AI integration with hybrid providers
- **Phase 3-5**: High-performance vector generation and backfill
